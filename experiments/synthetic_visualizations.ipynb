{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07faec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import *\n",
    "from models import *\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['axes.labelsize'] = 30\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.edgecolor'] = 'black'\n",
    "plt.rcParams['axes.linewidth'] = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df1 = pd.DataFrame()\n",
    "for ratio in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "    for seed in range(0, 80):\n",
    "        df = pd.read_csv(f\"../results/synthetic_results_A_{ratio}_{seed}.csv\")\n",
    "        results_df1 = pd.concat([results_df1, df], ignore_index=True)\n",
    "results_df1 = results_df1.drop(columns=['setting'])\n",
    "tarnet1 = results_df1[results_df1['model'] == 'TARNet']\n",
    "x_learner1 = results_df1[results_df1['model'] == 'X_learner']\n",
    "h_learner1 = results_df1[(results_df1['model'] == 'H_learner (X)') & (results_df1['reg_lambda'].isna())]\n",
    "\n",
    "results_df2 = pd.DataFrame()\n",
    "for ratio in [0.2,0.3,0.4,0.5]:\n",
    "    for seed in range(0, 80):\n",
    "        df = pd.read_csv(f\"../results/synthetic_results_B_{ratio}_{seed}.csv\")\n",
    "        results_df2 = pd.concat([results_df2, df], ignore_index=True)\n",
    "results_df2 = results_df2.drop(columns=['setting'])\n",
    "tarnet2 = results_df2[results_df2['model'] == 'TARNet']\n",
    "x_learner2 = results_df2[results_df2['model'] == 'X_learner']\n",
    "h_learner2 = results_df2[(results_df2['model'] == 'H_learner (X)') & (results_df2['reg_lambda'].isna())]\n",
    "\n",
    "results_df3 = pd.DataFrame()\n",
    "for ratio in [0.0,0.2,0.4,0.6,0.8]:\n",
    "    for seed in range(0, 80):\n",
    "        df = pd.read_csv(f\"../results/synthetic_results_C_{ratio}_{seed}.csv\")\n",
    "        results_df3 = pd.concat([results_df3, df], ignore_index=True)\n",
    "results_df3 = results_df3.drop(columns=['setting'])\n",
    "tarnet3 = results_df3[results_df3['model'] == 'TARNet']\n",
    "x_learner3 = results_df3[results_df3['model'] == 'X_learner']\n",
    "h_learner3 = results_df3[(results_df3['model'] == 'H_learner (X)') & (results_df3['reg_lambda'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6be76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=False)\n",
    "\n",
    "ratios1 = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "axs[0].plot(ratios1, tarnet1.groupby(['model', 'ratio']).mean()[\"pehe_test\"].values, 'o-', label='Tarnet')\n",
    "axs[0].plot(ratios1, x_learner1.groupby(['model', 'ratio']).mean()[\"pehe_test\"].values, 'o-', label='X-learner')\n",
    "axs[0].plot(ratios1, h_learner1.groupby(['model', 'ratio']).mean()[\"pehe_test\"].values, 'o-', label='H-learner')\n",
    "axs[0].set_xlabel('Proportion of Shared Features', fontsize=20)\n",
    "axs[0].set_ylabel(r'$\\epsilon_{PEHE}$', fontsize=20)\n",
    "axs[0].set_xticks(ratios1)\n",
    "axs[0].set_xticklabels([\"10%\", \"30%\", \"50%\", \"70%\", \"90%\"])\n",
    "axs[0].tick_params(axis='x', labelsize=16)\n",
    "axs[0].tick_params(axis='y', labelsize=16)\n",
    "axs[0].set_title(\"Setup A\", fontsize=20)\n",
    "\n",
    "ratios2 = [0.2, 0.3, 0.4, 0.5]\n",
    "axs[1].plot(ratios2, tarnet2.groupby(['model', 'ratio']).mean()[\"pehe_test\"].values, 'o-', label='Tarnet')\n",
    "axs[1].plot(ratios2, x_learner2.groupby(['model', 'ratio']).mean()[\"pehe_test\"].values, 'o-', label='X-learner')\n",
    "axs[1].plot(ratios2, h_learner2.groupby(['model', 'ratio']).mean()[\"pehe_test\"].values, 'o-', label='H-learner')\n",
    "axs[1].set_xlabel('Percentage of Treated Units', fontsize=20)\n",
    "axs[1].set_xticks(ratios2)\n",
    "axs[1].set_xticklabels([\"20%\", \"30%\", \"40%\", \"50%\"])\n",
    "axs[1].tick_params(axis='x', labelsize=16)\n",
    "axs[1].tick_params(axis='y', labelsize=16)\n",
    "axs[1].set_title(\"Setup B\", fontsize=20)\n",
    "\n",
    "ratios3 = [0, 0.2, 0.4, 0.6, 0.8]\n",
    "axs[2].plot(ratios3, tarnet3.groupby(['model', 'ratio']).mean()[\"pehe_test\"].values, 'o-', label='Tarnet')\n",
    "axs[2].plot(ratios3, x_learner3.groupby(['model', 'ratio']).mean()[\"pehe_test\"].values, 'o-', label='X-learner')\n",
    "axs[2].plot(ratios3, h_learner3.groupby(['model', 'ratio']).mean()[\"pehe_test\"].values, 'o-', label='H-learner')\n",
    "axs[2].set_xlabel(r'Degree of Confounding ($1-\\reg_lambda$)', fontsize=20)\n",
    "axs[2].set_xticks(ratios3)\n",
    "axs[2].set_xticklabels([\"1.0\", \"0.8\", \"0.6\", \"0.4\", \"0.2\"])\n",
    "axs[2].invert_xaxis()\n",
    "axs[2].tick_params(axis='x', labelsize=16)\n",
    "axs[2].tick_params(axis='y', labelsize=16)\n",
    "axs[2].legend(fontsize=18)\n",
    "axs[2].set_title(\"Setup C\", fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62df8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_low1 = pd.DataFrame()\n",
    "ratio = 0.1\n",
    "for seed in range(0, 80):\n",
    "    df = pd.read_csv(f\"../results/synthetic_results_A_{ratio}_{seed}.csv\")\n",
    "    results_df_low1 = pd.concat([results_df_low1, df], ignore_index=True)\n",
    "results_df_low1 = results_df_low1[results_df_low1['reg_lambda'].notna()]\n",
    "results_df_low1 = results_df_low1.drop(columns=['setting'])\n",
    "\n",
    "results_df_medium1 = pd.DataFrame()\n",
    "ratio = 0.5\n",
    "for seed in range(0, 80):\n",
    "    df = pd.read_csv(f\"../results/synthetic_results_A_{ratio}_{seed}.csv\")\n",
    "    results_df_medium1 = pd.concat([results_df_medium1, df], ignore_index=True)\n",
    "results_df_medium1 = results_df_medium1[results_df_medium1['reg_lambda'].notna()]\n",
    "results_df_medium1 = results_df_medium1.drop(columns=['setting'])\n",
    "\n",
    "results_df_high1 = pd.DataFrame()\n",
    "ratio = 0.9\n",
    "for seed in range(0, 80):\n",
    "    df = pd.read_csv(f\"../results/synthetic_results_A_{ratio}_{seed}.csv\")\n",
    "    results_df_high1 = pd.concat([results_df_high1, df], ignore_index=True)\n",
    "results_df_high1 = results_df_high1[results_df_high1['reg_lambda'].notna()]\n",
    "results_df_high1 = results_df_high1.drop(columns=['setting'])\n",
    "\n",
    "h_learner_low_performance1 = results_df_low1.groupby(['model', 'reg_lambda']).mean().reset_index()\n",
    "h_learner_medium_performance1 = results_df_medium1.groupby(['model', 'reg_lambda']).mean().reset_index()\n",
    "h_learner_high_performance1 = results_df_high1.groupby(['model', 'reg_lambda']).mean().reset_index()\n",
    "\n",
    "\n",
    "results_df_low2 = pd.DataFrame()\n",
    "ratio = 0.2\n",
    "for seed in range(0, 80):\n",
    "    df = pd.read_csv(f\"../results/synthetic_results_B_{ratio}_{seed}.csv\")\n",
    "    results_df_low2 = pd.concat([results_df_low2, df], ignore_index=True)\n",
    "results_df_low2 = results_df_low2[results_df_low2['reg_lambda'].notna()]\n",
    "results_df_low2 = results_df_low2.drop(columns=['setting'])\n",
    "\n",
    "results_df_medium2 = pd.DataFrame()\n",
    "ratio = 0.4\n",
    "for seed in range(0, 80):\n",
    "    df = pd.read_csv(f\"../results/synthetic_results_B_{ratio}_{seed}.csv\")\n",
    "    results_df_medium2 = pd.concat([results_df_medium2, df], ignore_index=True)\n",
    "results_df_medium2 = results_df_medium2[results_df_medium2['reg_lambda'].notna()]\n",
    "results_df_medium2 = results_df_medium2.drop(columns=['setting'])\n",
    "\n",
    "results_df_high2 = pd.DataFrame()\n",
    "ratio = 0.5\n",
    "for seed in range(0, 80):\n",
    "    df = pd.read_csv(f\"../results/synthetic_results_B_{ratio}_{seed}.csv\")\n",
    "    results_df_high2 = pd.concat([results_df_high2, df], ignore_index=True)\n",
    "results_df_high2 = results_df_high2[results_df_high2['reg_lambda'].notna()]\n",
    "results_df_high2 = results_df_high2.drop(columns=['setting'])\n",
    "\n",
    "h_learner_low_performance2 = results_df_low2.groupby(['model', 'reg_lambda']).mean().reset_index()\n",
    "h_learner_medium_performance2 = results_df_medium2.groupby(['model', 'reg_lambda']).mean().reset_index()\n",
    "h_learner_high_performance2 = results_df_high2.groupby(['model', 'reg_lambda']).mean().reset_index()\n",
    "\n",
    "results_df_low3 = pd.DataFrame()\n",
    "ratio = 0.2\n",
    "for seed in range(0, 80):\n",
    "    df = pd.read_csv(f\"../results/synthetic_results_C_{ratio}_{seed}.csv\")\n",
    "    results_df_low3 = pd.concat([results_df_low3, df], ignore_index=True)\n",
    "results_df_low3 = results_df_low3[results_df_low3['reg_lambda'].notna()]\n",
    "results_df_low3 = results_df_low3.drop(columns=['setting'])\n",
    "\n",
    "results_df_medium3 = pd.DataFrame()\n",
    "ratio = 0.4\n",
    "for seed in range(0, 80):\n",
    "    df = pd.read_csv(f\"../results/synthetic_results_C_{ratio}_{seed}.csv\")\n",
    "    results_df_medium3 = pd.concat([results_df_medium3, df], ignore_index=True)\n",
    "results_df_medium3 = results_df_medium3[results_df_medium3['reg_lambda'].notna()]\n",
    "results_df_medium3 = results_df_medium3.drop(columns=['setting'])\n",
    "\n",
    "results_df_high3 = pd.DataFrame()\n",
    "ratio = 0.8\n",
    "for seed in range(0, 80):\n",
    "    df = pd.read_csv(f\"../results/synthetic_results_C_{ratio}_{seed}.csv\")\n",
    "    results_df_high3 = pd.concat([results_df_high3, df], ignore_index=True)\n",
    "results_df_high3 = results_df_high3[results_df_high3['reg_lambda'].notna()]\n",
    "results_df_high3 = results_df_high3.drop(columns=['setting'])\n",
    "\n",
    "h_learner_low_performance3 = results_df_low3.groupby(['model', 'reg_lambda']).mean().reset_index()\n",
    "h_learner_medium_performance3 = results_df_medium3.groupby(['model', 'reg_lambda']).mean().reset_index()\n",
    "h_learner_high_performance3 = results_df_high3.groupby(['model', 'reg_lambda']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ba96af",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_low1 = h_learner_low_performance1[h_learner_low_performance1['reg_lambda'] == 0]['pehe_test'].values[0]\n",
    "h_learner_low_performance1['relative_pehe'] = h_learner_low_performance1['pehe_test'] / baseline_low1\n",
    "\n",
    "baseline_medium1 = h_learner_medium_performance1[h_learner_medium_performance1['reg_lambda'] == 0]['pehe_test'].values[0]\n",
    "h_learner_medium_performance1['relative_pehe'] = h_learner_medium_performance1['pehe_test'] / baseline_medium1\n",
    "\n",
    "baseline_high1 = h_learner_high_performance1[h_learner_high_performance1['reg_lambda'] == 0]['pehe_test'].values[0]\n",
    "h_learner_high_performance1['relative_pehe'] = h_learner_high_performance1['pehe_test'] / baseline_high1\n",
    "\n",
    "df_low1 = h_learner_low_performance1.sort_values('reg_lambda')\n",
    "df_medium1 = h_learner_medium_performance1.sort_values('reg_lambda')\n",
    "df_high1 = h_learner_high_performance1.sort_values('reg_lambda')\n",
    "\n",
    "baseline_low2 = h_learner_low_performance2[h_learner_low_performance2['reg_lambda'] == 0]['pehe_test'].values[0]\n",
    "h_learner_low_performance2['relative_pehe'] = h_learner_low_performance2['pehe_test'] / baseline_low2\n",
    "\n",
    "baseline_medium2 = h_learner_medium_performance2[h_learner_medium_performance2['reg_lambda'] == 0]['pehe_test'].values[0]\n",
    "h_learner_medium_performance2['relative_pehe'] = h_learner_medium_performance2['pehe_test'] / baseline_medium2\n",
    "\n",
    "baseline_high2 = h_learner_high_performance2[h_learner_high_performance2['reg_lambda'] == 0]['pehe_test'].values[0]\n",
    "h_learner_high_performance2['relative_pehe'] = h_learner_high_performance2['pehe_test'] / baseline_high2\n",
    "\n",
    "df_low2 = h_learner_low_performance2.sort_values('reg_lambda')\n",
    "df_medium2 = h_learner_medium_performance2.sort_values('reg_lambda')\n",
    "df_high2 = h_learner_high_performance2.sort_values('reg_lambda')\n",
    "\n",
    "baseline_low3 = h_learner_low_performance3[h_learner_low_performance3['reg_lambda'] == 0]['pehe_test'].values[0]\n",
    "h_learner_low_performance3['relative_pehe'] = h_learner_low_performance3['pehe_test'] / baseline_low3\n",
    "\n",
    "baseline_medium3 = h_learner_medium_performance3[h_learner_medium_performance3['reg_lambda'] == 0]['pehe_test'].values[0]\n",
    "h_learner_medium_performance3['relative_pehe'] = h_learner_medium_performance3['pehe_test'] / baseline_medium3\n",
    "\n",
    "baseline_high3 = h_learner_high_performance3[h_learner_high_performance3['reg_lambda'] == 0]['pehe_test'].values[0]\n",
    "h_learner_high_performance3['relative_pehe'] = h_learner_high_performance3['pehe_test'] / baseline_high3\n",
    "\n",
    "df_low3 = h_learner_low_performance3.sort_values('reg_lambda')\n",
    "df_medium3 = h_learner_medium_performance3.sort_values('reg_lambda')\n",
    "df_high3 = h_learner_high_performance3.sort_values('reg_lambda')\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16, 5))\n",
    "titles = ['Setup A', 'Setup B', 'Setup C']\n",
    "dfs = [\n",
    "    (df_low1, df_medium1, df_high1),\n",
    "    (df_low2, df_medium2, df_high2),\n",
    "    (df_low3, df_medium3, df_high3)\n",
    "]\n",
    "\n",
    "legend_labels = [\n",
    "    ['Low Shared Features', 'Medium Shared Features', 'High Shared Features'],\n",
    "    ['20% Treated (Imbalanced)', '40% Treated', '50% Treated (Balanced)'],\n",
    "    ['Low Confounding', 'Medium Confounding', 'High Confounding']\n",
    "]\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    df_low, df_medium, df_high = dfs[i]\n",
    "    labels = legend_labels[i]\n",
    "\n",
    "    ax.plot(df_high['reg_lambda'], df_high['relative_pehe'], marker='o', color='blue', label=labels[2])\n",
    "    ax.plot(df_medium['reg_lambda'], df_medium['relative_pehe'], marker='^', color='green', label=labels[1])\n",
    "    ax.plot(df_low['reg_lambda'], df_low['relative_pehe'], marker='s', color='red', label=labels[0])\n",
    "\n",
    "    ax.set_title(titles[i], fontsize=20)\n",
    "    ax.set_xlabel(r'Regularization Balance $\\lambda$', fontsize=20)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(r'$\\epsilon_{\\mathrm{PEHE}}$ relative to $\\lambda = 0$', fontsize=20)\n",
    "    ax.set_xticks([round(x * 0.1, 1) for x in range(11)])\n",
    "    ax.set_ylim(0.9, 1.10)\n",
    "    ax.tick_params(axis='y', labelsize=16)\n",
    "    ax.tick_params(axis='x', labelsize=16)\n",
    "    ax.legend(fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
